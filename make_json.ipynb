{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as FT\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# label_map\n",
    "voc_labels = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n",
    "              'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "\n",
    "label_map = {k: v + 1 for v, k in enumerate(voc_labels)}\n",
    "label_map['background'] = 0\n",
    "rev_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Color map for bounding boxes of detected objects from https://sashat.me/2017/01/11/list-of-20-simple-distinct-colors/\n",
    "distinct_colors = ['#e6194b', '#3cb44b', '#ffe119', '#0082c8', '#f58231', '#911eb4', '#46f0f0', '#f032e6',\n",
    "                   '#d2f53c', '#fabebe', '#008080', '#000080', '#aa6e28', '#fffac8', '#800000', '#aaffc3', '#808000',\n",
    "                   '#ffd8b1', '#e6beff', '#808080', '#FFFFFF']\n",
    "\n",
    "label_color_map = {k: distinct_colors[i] for i, k in enumerate(label_map.keys())}\n",
    "\n",
    "\n",
    "def parse_annotation(annotation_path):\n",
    "    \"\"\"Parse Annotation\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    \n",
    "\n",
    "    Return: dict\n",
    "    ------\n",
    "    boxes: a list of list\n",
    "    labels: a list\n",
    "    difficulties: a list\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "    difficulties = list()\n",
    "    for object in root.iter('object'):\n",
    "\n",
    "        difficult = int(object.find('difficult').text == '1')\n",
    "\n",
    "        label = object.find('name').text.lower().strip()\n",
    "        if label not in label_map:\n",
    "            continue\n",
    "\n",
    "        bbox = object.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text) - 1\n",
    "        ymin = int(bbox.find('ymin').text) - 1\n",
    "        xmax = int(bbox.find('xmax').text) - 1\n",
    "        ymax = int(bbox.find('ymax').text) - 1\n",
    "\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        labels.append(label_map[label])\n",
    "        difficulties.append(difficult)\n",
    "\n",
    "    return {'boxes': boxes, 'labels': labels, 'difficulties': difficulties}\n",
    "\n",
    "\n",
    "def create_data_lists(voc07_path, output_folder):\n",
    "    \"\"\"\n",
    "    Create lists of images, the bounding boxes and labels of the objects in these images, and save these to file.\n",
    "\n",
    "    :param voc07_path: path to the 'VOC2007' folder\n",
    "    ex) voc07_path = /data/yjkim/data/pascal/VOCdevkit/VOC2007\n",
    "    \n",
    "    :param voc12_path: path to the 'VOC2012' folder\n",
    "    ex) voc12_path = /data/yjkim/data/pascal/VOCdevkit/VOC2012\n",
    "\n",
    "    :param output_folder: folder where the JSONs must be saved\n",
    "    ex) /data/yjkim/data/pascal/VOCdevkit/VOC2007/jsonlist\n",
    "    \"\"\"\n",
    "    voc07_path = os.path.abspath('/disk1/jtku/dev/od/data/VOCdevkit/VOC2007')\n",
    "   # output_folder = os.path.abspath('/home/jtku/dev/od/data/VOCdevkit/VOC2007/jsonlist')\n",
    "    train_images = list()\n",
    "    train_objects = list()\n",
    "    n_objects = 0\n",
    "\n",
    "    # Training data\n",
    "    for path in [voc07_path]:\n",
    "\n",
    "        # Find IDs of images in training data\n",
    "        with open(os.path.join(path, 'ImageSets/Main/trainval.txt')) as f:\n",
    "            ids = f.read().splitlines()\n",
    "\n",
    "        for id in ids:\n",
    "            # Parse annotation's XML file\n",
    "            objects = parse_annotation(os.path.join(path, 'Annotations', id + '.xml'))\n",
    "            if len(objects) == 0:\n",
    "                continue\n",
    "            n_objects += len(objects)\n",
    "            train_objects.append(objects)\n",
    "            train_images.append(os.path.join(path, 'JPEGImages', id + '.jpg'))\n",
    "\n",
    "    assert len(train_objects) == len(train_images)\n",
    "\n",
    "    # Save to file\n",
    "    with open(os.path.join(output_folder, 'TRAIN_images.json'), 'w') as j:\n",
    "        json.dump(train_images, j)\n",
    "    with open(os.path.join(output_folder, 'TRAIN_objects.json'), 'w') as j:\n",
    "        json.dump(train_objects, j)\n",
    "    with open(os.path.join(output_folder, 'label_map.json'), 'w') as j:\n",
    "        json.dump(label_map, j)  # save label map too\n",
    "\n",
    "    print('\\nThere are %d training images containing a total of %d objects. Files have been saved to %s.' % (\n",
    "        len(train_images), n_objects, os.path.abspath(output_folder)))\n",
    "\n",
    "    # Validation data\n",
    "    test_images = list()\n",
    "    test_objects = list()\n",
    "    n_objects = 0\n",
    "\n",
    "    # Find IDs of images in validation data\n",
    "    with open(os.path.join(voc07_path, '/disk1/jtku/dev/od/data/VOCdevkit/VOC2007/ImageSets/Main/test.txt')) as f:\n",
    "        ids = f.read().splitlines()\n",
    "\n",
    "    for id in ids:\n",
    "        # Parse annotation's XML file\n",
    "        objects = parse_annotation(os.path.join(voc07_path, '/disk1/jtku/dev/od/data/VOCdevkit/VOC2007/Annotations', id + '.xml'))\n",
    "        if len(objects) == 0:\n",
    "            continue\n",
    "        test_objects.append(objects)\n",
    "        n_objects += len(objects)\n",
    "        test_images.append(os.path.join(voc07_path, '/disk1/jtku/dev/od/data/VOCdevkit/VOC2007/JPEGImages', id + '.jpg'))\n",
    "\n",
    "    assert len(test_objects) == len(test_images)\n",
    "\n",
    "    # Save to file\n",
    "    with open(os.path.join(output_folder, 'TEST_images.json'), 'w') as j:\n",
    "        json.dump(test_images, j)\n",
    "    with open(os.path.join(output_folder, 'TEST_objects.json'), 'w') as j:\n",
    "        json.dump(test_objects, j)\n",
    "\n",
    "    print('\\nThere are %d validation images containing a total of %d objects. Files have been saved to %s.' % (\n",
    "        len(test_images), n_objects, os.path.abspath(output_folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 5011 training images containing a total of 15033 objects. Files have been saved to /disk1/jtku/dev/od/data/VOCdevkit/VOC2007/jsonlist.\n",
      "\n",
      "There are 4952 validation images containing a total of 14856 objects. Files have been saved to /disk1/jtku/dev/od/data/VOCdevkit/VOC2007/jsonlist.\n"
     ]
    }
   ],
   "source": [
    "create_data_lists('/disk1/jtku/dev/od/data/VOCdevkit/VOC2007', '/disk1/jtku/dev/od/data/VOCdevkit/VOC2007/jsonlist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LITS MAKE_JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b60070bc5fdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Class & box color match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mlabel_color_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdistinct_colors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-b60070bc5fdc>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Class & box color match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mlabel_color_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdistinct_colors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as FT\n",
    "\n",
    "#cup 지정해주기?\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# label_map\n",
    "voc_labels = ('background', 'tumor')\n",
    "\n",
    "voc_labels = ('tumor','background')\n",
    "\n",
    "\n",
    "label_map = {k: v + 1 for v, k in enumerate(voc_labels)}\n",
    "print(label_map)\n",
    "# 배경 키 값 0으로 지정\n",
    "label_map['background'] = 0\n",
    "print(label_map)\n",
    "rev_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Color map for bounding boxes of detected objects from https://sashat.me/2017/01/11/list-of-20-simple-distinct-colors/\n",
    "distinct_colors = ['#e6194b', '#FFFFFF']\n",
    "\n",
    "# Class & box color match\n",
    "label_color_map = {k: distinct_colors[i] for i, k in enumerate(label_map.keys())}\n",
    "\n",
    "\n",
    "def parse_annotation(annotation_path): \n",
    "    \"\"\"Parse Annotation\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    \n",
    "\n",
    "    Return: dict\n",
    "    ------\n",
    "    boxes: a list of list\n",
    "    labels: a list\n",
    "    difficulties: a list\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "    difficulties = list()\n",
    "    \n",
    "    for object in root.iter('object'):\n",
    "\n",
    "        difficult = int(object.find('difficult').text == '1')\n",
    "\n",
    "        label = object.find('name').text.lower().strip()\n",
    "        if label not in label_map:\n",
    "            continue\n",
    "\n",
    "        bbox = object.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text) - 1\n",
    "        ymin = int(bbox.find('ymin').text) - 1\n",
    "        xmax = int(bbox.find('xmax').text) - 1\n",
    "        ymax = int(bbox.find('ymax').text) - 1\n",
    "\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        labels.append(label_map[label])\n",
    "        difficulties.append(difficult)\n",
    "\n",
    "    return {'boxes': boxes, 'labels': labels, 'difficulties': difficulties}\n",
    "\n",
    "\n",
    "def create_data_lists(mask_path, output_folder):\n",
    "    \"\"\"\n",
    "    Create lists of images, the bounding boxes and labels of the objects in these images, and save these to file.\n",
    "\n",
    "    :param voc07_path: path to the 'VOC2007' folder\n",
    "    ex) voc07_path = /data/yjkim/data/pascal/VOCdevkit/VOC2007\n",
    "    \n",
    "    :param voc12_path: path to the 'VOC2012' folder\n",
    "    ex) voc12_path = /data/yjkim/data/pascal/VOCdevkit/VOC2012\n",
    "\n",
    "    :param output_folder: folder where the JSONs must be saved\n",
    "    ex) /data/yjkim/data/pascal/VOCdevkit/VOC2007/jsonlist\n",
    "    \"\"\"\n",
    "    mask_path = os.path.abspath('/disk1/jtku/dev/mask/xml')\n",
    "   # output_folder = os.path.abspath('/home/jtku/dev/od/data/VOCdevkit/VOC2007/jsonlist')\n",
    "    train_images = list()\n",
    "    train_objects = list()\n",
    "    n_objects = 0\n",
    "\n",
    "    # Training data\n",
    "    for path in [mask_path]:\n",
    "\n",
    "        # Find IDs of images in training data\n",
    "        with open(os.path.join(path, 'ImageSets/Main/trainval.txt')) as f:\n",
    "            ids = f.read().splitlines()\n",
    "\n",
    "        for id in ids:\n",
    "            # Parse annotation's XML file\n",
    "            objects = parse_annotation(os.path.join(path, 'Annotations', id + '.xml'))\n",
    "            if len(objects) == 0:\n",
    "                continue\n",
    "            n_objects += len(objects)\n",
    "            train_objects.append(objects)\n",
    "            train_images.append(os.path.join(path, 'JPEGImages', id + '.jpg'))\n",
    "\n",
    "    assert len(train_objects) == len(train_images)\n",
    "\n",
    "    # Save to file\n",
    "    with open(os.path.join(output_folder, 'TRAIN_images.json'), 'w') as j:\n",
    "        json.dump(train_images, j)\n",
    "    with open(os.path.join(output_folder, 'TRAIN_objects.json'), 'w') as j:\n",
    "        json.dump(train_objects, j)\n",
    "    with open(os.path.join(output_folder, 'label_map.json'), 'w') as j:\n",
    "        json.dump(label_map, j)  # save label map too\n",
    "\n",
    "    print('\\nThere are %d training images containing a total of %d objects. Files have been saved to %s.' % (\n",
    "        len(train_images), n_objects, os.path.abspath(output_folder)))\n",
    "\n",
    "    # Validation data\n",
    "    test_images = list()\n",
    "    test_objects = list()\n",
    "    n_objects = 0\n",
    "\n",
    "    # Find IDs of images in validation data\n",
    "    with open(os.path.join(voc07_path, '/disk1/jtku/dev/od/data/VOCdevkit/VOC2007/ImageSets/Main/test.txt')) as f:\n",
    "        ids = f.read().splitlines()\n",
    "\n",
    "    for id in ids:\n",
    "        # Parse annotation's XML file\n",
    "        objects = parse_annotation(os.path.join(voc07_path, '/disk1/jtku/dev/od/data/VOCdevkit/VOC2007/Annotations', id + '.xml'))\n",
    "        if len(objects) == 0:\n",
    "            continue\n",
    "        test_objects.append(objects)\n",
    "        n_objects += len(objects)\n",
    "        test_images.append(os.path.join(voc07_path, '/disk1/jtku/dev/od/data/VOCdevkit/VOC2007/JPEGImages', id + '.jpg'))\n",
    "\n",
    "    assert len(test_objects) == len(test_images)\n",
    "\n",
    "    # Save to file\n",
    "    with open(os.path.join(output_folder, 'TEST_images.json'), 'w') as j:\n",
    "        json.dump(test_images, j)\n",
    "    with open(os.path.join(output_folder, 'TEST_objects.json'), 'w') as j:\n",
    "        json.dump(test_objects, j)\n",
    "\n",
    "    print('\\nThere are %d validation images containing a total of %d objects. Files have been saved to %s.' % (\n",
    "        len(test_images), n_objects, os.path.abspath(output_folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d31eb69805e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'root' is not defined"
     ]
    }
   ],
   "source": [
    "for object in root.iter('object'):\n",
    "    print(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tumor': 1, 'background': 2}\n",
      "{'tumor': 1, 'background': 0}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voc_lavels : ('tumor', 'background')\n",
      "\n",
      "\n",
      "label_map : {'tumor': 1, 'background': 0}\n",
      "\n",
      "\n",
      "rev_label_map : {1: 'tumor', 0: 'background'}\n",
      "\n",
      "\n",
      "distinct_colors : ['#e6194b', '#FFFFFF']\n",
      "\n",
      "\n",
      "label_color_map : {'tumor': '#e6194b', 'background': '#FFFFFF'}\n"
     ]
    }
   ],
   "source": [
    "print(\"voc_lavels :\", voc_labels)\n",
    "\n",
    "print(\"\\n\\nlabel_map :\",label_map)\n",
    "\n",
    "print(\"\\n\\nrev_label_map :\",rev_label_map)\n",
    "\n",
    "print(\"\\n\\ndistinct_colors :\",distinct_colors)\n",
    "\n",
    "print(\"\\n\\nlabel_color_map :\",label_color_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_labels = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n",
    "              'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "\n",
    "label_map = {k: v + 1 for v, k in enumerate(voc_labels)}\n",
    "label_map['background'] = 0\n",
    "rev_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Color map for bounding boxes of detected objects from https://sashat.me/2017/01/11/list-of-20-simple-distinct-colors/\n",
    "distinct_colors = ['#e6194b', '#3cb44b', '#ffe119', '#0082c8', '#f58231', '#911eb4', '#46f0f0', '#f032e6',\n",
    "                   '#d2f53c', '#fabebe', '#008080', '#000080', '#aa6e28', '#fffac8', '#800000', '#aaffc3', '#808000',\n",
    "                   '#ffd8b1', '#e6beff', '#808080', '#FFFFFF']\n",
    "\n",
    "label_color_map = {k: distinct_colors[i] for i, k in enumerate(label_map.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multi-byte encodings are not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f536c2cbbcf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mxmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXMLParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"EUC-KR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/disk1/jtku/dev/mask/xml/000.xml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxmlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, parser)\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \"\"\"\n\u001b[1;32m   1195\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElementTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, parser)\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                 \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: multi-byte encodings are not supported"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "from medpy.io import load, save\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.etree.ElementTree import parse\n",
    "\n",
    "xmlp = ET.XMLParser(encoding=\"EUC-KR\")\n",
    "\n",
    "tree = ET.parse('/disk1/jtku/dev/mask/xml/000.xml', parser=xmlp)\n",
    "\n",
    "root = tree.getroot()\n",
    "\n",
    "SegmentationBoxList = root.findall(\"SegmentationBoxList\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 75)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "img = nib.load(\"/disk1/jtku/dev/mask/convert/segmentation-0.nii\")\n",
    "\n",
    "img.shape\n",
    "\n",
    "\n",
    "\n",
    "# img_data = img.get_data()\n",
    "\n",
    "# img_data_arr = np.asarray(img_data)\n",
    "\n",
    "# print(img_data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multi-byte encodings are not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a08b74e07a8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/disk1/jtku/dev/mask/xml/000.xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb'\\xc4\\xda\\xbd\\xba\\xc7\\xc7\\xc1\\xf6\\xbc\\xf6'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, parser)\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \"\"\"\n\u001b[1;32m   1195\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElementTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, parser)\u001b[0m\n\u001b[1;32m    595\u001b[0m                     \u001b[0;31m# It can be used to parse the whole source without feeding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0;31m# it with chunks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_whole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: multi-byte encodings are not supported"
     ]
    }
   ],
   "source": [
    "from xml.etree.ElementTree import parse\n",
    "\n",
    "\n",
    "\n",
    "tree = parse('/disk1/jtku/dev/mask/xml/000.xml')\n",
    "\n",
    "s = b'\\xc4\\xda\\xbd\\xba\\xc7\\xc7\\xc1\\xf6\\xbc\\xf6'\n",
    "type(s), s \n",
    "\n",
    "u = s.decode('euc-kr')\n",
    "type(u), u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final_make_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Making a json file\n",
    "from od.utils import create_data_lists\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "HCC_PATH = \"/disk1/jtku/dev/final\"\n",
    "OUTPUT_PATH = os.path.join(HCC_PATH, \"jsonpath\")\n",
    "\n",
    "\n",
    "# paths // 폴더 안의 파일들 path list 불러오기\n",
    "dcm_paths = glob.glob(os.path.join(HCC_PATH,\"dcmImages\",\"*.dcm\"))\n",
    "label_map = {\"background\":0,\"hcc\":1}\n",
    "\n",
    "def hcc_parse_annotation(annotation_path):\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "    \n",
    "    for object in root.iter('object'):\n",
    "        label = \"hcc\"\n",
    "        if label not in label_map:\n",
    "            continue\n",
    "        bbox = object.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text) - 1\n",
    "        ymin = int(bbox.find('ymin').text) - 1\n",
    "        xmax = int(bbox.find('xmax').text) - 1\n",
    "        ymax = int(bbox.find('ymax').text) - 1\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        labels.append(label_map[label])\n",
    "    \n",
    "    return {'boxes': boxes, 'labels': labels}\n",
    "def create_hcc_data_lists(paths,  output_folder):\n",
    "    paths = os.path.abspath(paths)\n",
    "    #voc12_path = os.path.abspath(voc12_path)\n",
    "    train_images = list()\n",
    "    train_objects = list()\n",
    "    # Training data\n",
    "    for path in dcm_paths[:-148]:\n",
    "        file = path.split(sep=\"/\")[-1] # 맨마지막\n",
    "        idx = file.replace(\".dcm\",\"\") # 확장자 제거\n",
    "        objects = hcc_parse_annotation(os.path.join(HCC_PATH, 'Annotations', idx + '.xml'))\n",
    "        train_objects.append(objects)\n",
    "        train_images.append(os.path.join(HCC_PATH, 'dcmImages', idx + '.dcm'))\n",
    "            \n",
    "    assert len(train_objects) == len(train_images), 'train_objects == train_images가 아니랸다 친구야'\n",
    "    \n",
    "    \n",
    "    # Save to file\n",
    "    with open(os.path.join(output_folder, 'TRAIN_images.json'), 'w') as j:\n",
    "        json.dump(train_images, j)\n",
    "    with open(os.path.join(output_folder, 'TRAIN_objects.json'), 'w') as j:\n",
    "        json.dump(train_objects, j)\n",
    "    with open(os.path.join(output_folder, 'label_map.json'), 'w') as j:\n",
    "        json.dump(label_map, j)  # save label map too\n",
    "    print('\\nThere are %d training images. Files have been saved to %s.' % (\n",
    "        len(train_images), os.path.abspath(output_folder)))\n",
    "    # Validation data\n",
    "    test_images = list()\n",
    "    test_objects = list()\n",
    "    \n",
    "    for path in dcm_paths[-148:]:\n",
    "        file = path.split(sep=\"/\")[-1] # 맨마지막\n",
    "        idx = file.replace(\".dcm\",\"\") # 확장자 제거\n",
    "        objects = hcc_parse_annotation(os.path.join(HCC_PATH, 'Annotations', idx + '.xml'))\n",
    "        test_objects.append(objects)\n",
    "        test_images.append(os.path.join(HCC_PATH, 'dcmImages', idx + '.dcm'))\n",
    "            \n",
    "    assert len(test_objects) == len(test_images)\n",
    "    # Save to file\n",
    "    with open(os.path.join(output_folder, 'TEST_images.json'), 'w') as j:\n",
    "        json.dump(test_images, j)\n",
    "    with open(os.path.join(output_folder, 'TEST_objects.json'), 'w') as j:\n",
    "        json.dump(test_objects, j)\n",
    "    print('\\nThere are %d validation images. Files have been saved to %s.' % (\n",
    "        len(test_images),os.path.abspath(output_folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 592 training images. Files have been saved to /disk1/jtku/dev/final/jsonpath.\n",
      "\n",
      "There are 148 validation images. Files have been saved to /disk1/jtku/dev/final/jsonpath.\n"
     ]
    }
   ],
   "source": [
    "create_hcc_data_lists('/disk1/jtku/dev/final/Annotations/', '/disk1/jtku/dev/final/jsonpath/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_loss = []\n",
    "train_accu = []\n",
    "i = 0\n",
    "for epoch in range(15):\n",
    "    for data, target in train_loader:\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()    # calc gradients\n",
    "        train_loss.append(loss.data[0])\n",
    "        optimizer.step()   # update gradients\n",
    "        prediction = output.data.max(1)[1]   # first column has actual prob.\n",
    "        accuracy = prediction.eq(target.data).sum()/batch_size*100\n",
    "        train_accu.append(accuracy)\n",
    "        if i % 1000 == 0:\n",
    "            print('Train Step: {}\\tLoss: {:.3f}\\tAccuracy: {:.3f}'.format(i, loss.data[0], accuracy))\n",
    "        i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
